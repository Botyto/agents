diff --git a/autogen/token_count_utils.py b/autogen/token_count_utils.py
index b71dbc428..e930129d8 100644
--- a/autogen/token_count_utils.py
+++ b/autogen/token_count_utils.py
@@ -8,35 +8,36 @@
 logger = logging.getLogger(__name__)
 
 
+max_token_limit = {
+    "gpt-3.5-turbo": 16385,
+    "gpt-3.5-turbo-0125": 16385,
+    "gpt-3.5-turbo-0301": 4096,
+    "gpt-3.5-turbo-0613": 4096,
+    "gpt-3.5-turbo-instruct": 4096,
+    "gpt-3.5-turbo-16k": 16385,
+    "gpt-3.5-turbo-16k-0613": 16385,
+    "gpt-3.5-turbo-1106": 16385,
+    "gpt-4": 8192,
+    "gpt-4-turbo": 128000,
+    "gpt-4-turbo-2024-04-09": 128000,
+    "gpt-4-32k": 32768,
+    "gpt-4-32k-0314": 32768,  # deprecate in Sep
+    "gpt-4-0314": 8192,  # deprecate in Sep
+    "gpt-4-0613": 8192,
+    "gpt-4-32k-0613": 32768,
+    "gpt-4-1106-preview": 128000,
+    "gpt-4-0125-preview": 128000,
+    "gpt-4-turbo-preview": 128000,
+    "gpt-4-vision-preview": 128000,
+    "gpt-4o": 128000,
+    "gpt-4o-2024-05-13": 128000,
+}
+
 def get_max_token_limit(model: str = "gpt-3.5-turbo-0613") -> int:
     # Handle common azure model names/aliases
     model = re.sub(r"^gpt\-?35", "gpt-3.5", model)
     model = re.sub(r"^gpt4", "gpt-4", model)
-
-    max_token_limit = {
-        "gpt-3.5-turbo": 16385,
-        "gpt-3.5-turbo-0125": 16385,
-        "gpt-3.5-turbo-0301": 4096,
-        "gpt-3.5-turbo-0613": 4096,
-        "gpt-3.5-turbo-instruct": 4096,
-        "gpt-3.5-turbo-16k": 16385,
-        "gpt-3.5-turbo-16k-0613": 16385,
-        "gpt-3.5-turbo-1106": 16385,
-        "gpt-4": 8192,
-        "gpt-4-turbo": 128000,
-        "gpt-4-turbo-2024-04-09": 128000,
-        "gpt-4-32k": 32768,
-        "gpt-4-32k-0314": 32768,  # deprecate in Sep
-        "gpt-4-0314": 8192,  # deprecate in Sep
-        "gpt-4-0613": 8192,
-        "gpt-4-32k-0613": 32768,
-        "gpt-4-1106-preview": 128000,
-        "gpt-4-0125-preview": 128000,
-        "gpt-4-turbo-preview": 128000,
-        "gpt-4-vision-preview": 128000,
-        "gpt-4o": 128000,
-        "gpt-4o-2024-05-13": 128000,
-    }
+    global max_token_limit
     return max_token_limit[model]
 
 
